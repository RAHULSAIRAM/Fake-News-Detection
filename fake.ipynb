{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#INDvENG\n",
      "1235449850654683141\n",
      "1235449848553299968\n",
      "1235449847441813504\n",
      "1235449839040606209\n",
      "USER: ursarvindnaani\n",
      "1235449836054253568\n",
      "USER: NewGenral12\n",
      "USER: insanesapien\n",
      "USER: pranav9983\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "#RIPTwitter\n",
      "1235450032742154240\n",
      "1235450032498896903\n",
      "1235450030544179201\n",
      "1235450030493814786\n",
      "1235450029801910272\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "#ThursdayThoughts\n",
      "1235450216074997767\n",
      "1235450212325314560\n",
      "1235450199998205952\n",
      "1235450197116928000\n",
      "1235450192784048128\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "#20YrsThatChangedOdisha\n",
      "1235450516945002496\n",
      "1235450511475609600\n",
      "1235450511194615808\n",
      "1235450499853193217\n",
      "1235450488931287040\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "#HBDSelvaraghavan\n",
      "1235450659081617408\n",
      "1235450624705097728\n",
      "1235450599606349824\n",
      "1235450545885851648\n",
      "1235450533126615040\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Yes Bank\n",
      "1235450833279397890\n",
      "1235450807698493445\n",
      "1235450779340660736\n",
      "1235450753575157760\n",
      "1235450749867266048\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "Harish Salve\n",
      "1235451153288056832\n",
      "1235451137605525504\n",
      "1235451132723355649\n",
      "1235451111445639168\n",
      "1235451074728738821\n",
      "|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\n",
      "पूर्व मुख्यमंत्री\n",
      "1235451033079279616\n",
      "1235450846256574471\n",
      "1235450837469556736\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "[{'message': 'Rate limit exceeded', 'code': 88}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c056ac96c87c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_timeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscreenname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#===> print(\"TWEET:\", tweet.text) <===  ###To get the content of the tweet ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mreTweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"USER:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreTweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen_name\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m###Total no.of users who retweeted the tweets ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m#print (tweet.created_at, tweet.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tweepy\\binder.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_rate_limit_error_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRateLimitError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTweepError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi_code\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mapi_error_code\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: [{'message': 'Rate limit exceeded', 'code': 88}]"
     ]
    }
   ],
   "source": [
    "\n",
    "                    ##### Extraction of tweets from twitter #####\n",
    "\n",
    "import tweepy as tp\n",
    "import json\n",
    "import sys\n",
    "import csv\n",
    "import blockspring\n",
    "import json\n",
    "\n",
    "### Here we are initializing our API key ###\n",
    "consumer_key = \"krklOEjpcPNJTFVf553xehNhi\"\n",
    "consumer_secret = \"qZtbTKGDYBwJ8yI0AuRvOlVyQa1e11g2RvjdRmQsc2LzLN7wlX\"\n",
    "access_key = \"1055369228130217984-9YPZH6SFCuAeQB0KnEAoXmcuY9x6Ax\"\n",
    "access_token_secret = \"PnAd2tIEXdIfJMPSjw7C2wRWKiEiqwwbzLYXpQgYryWI9\"\n",
    "\n",
    "### Creation of Authentication Object ###\n",
    "auth = tp.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "auth.set_access_token(access_key, access_token_secret)\n",
    "\n",
    "api = tp.API(auth)\n",
    "\n",
    "### Exatraction of Available Trends in Twitter ###\n",
    "public_tweets = api.trends_available()\n",
    "\n",
    "### Using India's WOEID code ###\n",
    "Ind_Id = 2282863\n",
    "\n",
    "\n",
    "### Extarcting available trends in India ###\n",
    "ind_trends = api.trends_place(Ind_Id)\n",
    "\n",
    "trends = json.loads(json.dumps(ind_trends, indent=1))\n",
    "\n",
    "for trend in trends[0][\"trends\"]:\n",
    "    print (trend[\"name\"])  ###Printing Trending Topics ###\n",
    "    csvFile = open('ua.csv', 'a')\n",
    "    #Use csv Writer\n",
    "    csvWriter = csv.writer(csvFile) \n",
    "    for tweet in tp.Cursor(api.search,q=trend[\"name\"],count=0,lang=\"en\").items(5):\n",
    "        print (tweet.id)   ### printing Tweet_ID ###\n",
    "        hey=api.get_status(tweet.id)  ### Getting status of a tweet ###\n",
    "        screenname=(hey.user.screen_name)   ### Getting Username of the Tweet owner or creator ###\n",
    "        for tweet in api.user_timeline(screen_name = screenname, count=2):\n",
    "        #===> print(\"TWEET:\", tweet.text) <===  ###To get the content of the tweet ###\n",
    "            for reTweet in api.retweets(tweet.id,5):\n",
    "                print(\"USER:\", reTweet.user.screen_name)   ###Total no.of users who retweeted the tweets ###\n",
    "        #print (tweet.created_at, tweet.text)\n",
    "                csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n",
    "    print(\"|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\VISHWANATH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mars', 'is', 'approximately', 'half', 'the', 'diameter', 'of', 'Earth', '.']\n",
      "['Mars is a cold desert world.', 'It is half the size of Earth.']\n",
      "Number of documents: 3\n",
      "[['mars', 'is', 'the', 'fourth', 'planet', 'in', 'our', 'solar', 'system', '.'], ['it', 'is', 'second-smallest', 'planet', 'in', 'the', 'solar', 'system', 'after', 'mercury', '.'], ['saturn', 'is', 'yellow', 'planet', '.']]\n",
      "{'.': 0, 'fourth': 1, 'in': 2, 'is': 3, 'mars': 4, 'our': 5, 'planet': 6, 'solar': 7, 'system': 8, 'the': 9, 'after': 10, 'it': 11, 'mercury': 12, 'second-smallest': 13, 'saturn': 14, 'yellow': 15}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)], [(0, 1), (2, 1), (3, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(0, 1), (3, 1), (6, 1), (14, 1), (15, 1)]]\n",
      "[['fourth', 0.53], ['in', 0.2], ['mars', 0.53], ['our', 0.53], ['solar', 0.2], ['system', 0.2], ['the', 0.2]]\n",
      "[['in', 0.17], ['solar', 0.17], ['system', 0.17], ['the', 0.17], ['after', 0.47], ['it', 0.47], ['mercury', 0.47], ['second-smallest', 0.47]]\n",
      "[['saturn', 0.71], ['yellow', 0.71]]\n",
      "Number of documents: 1\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "Number of documents: 1\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "0.78813386\n",
      "Average similarity float: 0.2627112865447998\n",
      "Average similarity percentage: 26.27112865447998\n",
      "Average similarity rounded percentage: 26\n",
      "Comparing Result: [0.11641413 0.10281226 0.56890744]\n",
      "avg: 0.2627112865447998\n",
      "\n",
      "Total_avg 0.2627112865447998\n",
      "percentage_of_Total_avg 26.27112865447998 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "                                ##### Natural Language Processing #####\n",
    "\n",
    "from tweepy import Stream\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re #regular expression\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = \"Mars is approximately half the diameter of Earth.\"\n",
    "print(word_tokenize(data))\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "data = \"Mars is a cold desert world. It is half the size of Earth. \"\n",
    "print(sent_tokenize(data))\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "file_docs = []\n",
    "with open ('DemoFile1.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file_docs.append(line)\n",
    "print(\"Number of documents:\",len(file_docs))\n",
    "\n",
    "\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)]\n",
    "            for text in file_docs]\n",
    "print (gen_docs)\n",
    "\n",
    "\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(dictionary.token2id)\n",
    "\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n",
    "\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "for doc in tf_idf[corpus]:\n",
    "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])\n",
    "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
    "                                        num_features=len(dictionary))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "file2_docs = []\n",
    "with open ('DemoFile3.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
    "\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "\n",
    "\n",
    "file2_docs = []\n",
    "with open ('DemoFile3.txt') as f:\n",
    "    tokens = sent_tokenize(f.read())\n",
    "    for line in tokens:\n",
    "        file2_docs.append(line)\n",
    "print(\"Number of documents:\",len(file2_docs))  \n",
    "for line in file2_docs:\n",
    "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "    query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and\n",
    "\n",
    "\n",
    "\n",
    "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "# print(document_number, document_similarity)\n",
    "print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "\n",
    "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "print(sum_of_sims)\n",
    "\n",
    "percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))\n",
    "print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')\n",
    "print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')\n",
    "print(f'Average similarity rounded percentage: {percentage_of_similarity}')\n",
    "\n",
    "avg_sims = [] # array of averages\n",
    "# for line in query documents\n",
    "for line in file2_docs:\n",
    "        # tokenize words\n",
    "        query_doc = [w.lower() for w in word_tokenize(line)]\n",
    "        # create bag of words\n",
    "        query_doc_bow = dictionary.doc2bow(query_doc)\n",
    "        # find similarity for each document\n",
    "        query_doc_tf_idf = tf_idf[query_doc_bow]\n",
    "        # print (document_number, document_similarity)\n",
    "        print('Comparing Result:', sims[query_doc_tf_idf])\n",
    "        # calculate sum of similarities for each query doc\n",
    "        sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
    "        # calculate average of similarity for each query doc\n",
    "        avg = sum_of_sims / len(file_docs)\n",
    "        # print average of similarity for each query doc\n",
    "        print(f'avg: {sum_of_sims / len(file_docs)}')    \n",
    "        # add average values into array\n",
    "        avg_sims.append(avg)  \n",
    "   # calculate total average\n",
    "        total_avg = np.sum(avg_sims,dtype=np.float)\n",
    "        print()\n",
    "        if total_avg >=100:\n",
    "          print(\"Total_avg = 100\")\n",
    "        else:\n",
    "             print(\"Total_avg\",total_avg)\n",
    "             print(\"percentage_of_Total_avg\",total_avg*100,\"%\")    \n",
    "    # round the value and multiply by 100 to format it as percentage\n",
    "        percentage_of_similarity = round(float(total_avg) * 100)\n",
    "    # if percentage is greater than 100\n",
    "    # that means documents are almost same\n",
    "        if  percentage_of_similarity >= 100:\n",
    "          percentage_of_similarity = 100\n",
    "          print(percentage_of_similarity,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
